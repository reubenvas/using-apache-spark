<div class="ql-editor bb-editor" data-gramm="false" contenteditable="false" id="bbml-editor-id_12dde3c9-5e85-4d6b-9c45-03b783545318-rte"><p>The third assignment consists of the construction of a predictive model using Spark (Structured) Streaming and textual data. You will work with data coming from&nbsp;<a href="https://news.ycombinator.com/" target="_blank" class="wants-props-update" tabindex="0" style="color: rgb(77, 143, 168);">Hacker News</a>, a community driven news aggregator for news that IT people, hackers, data scientists, technologists etc. find interesting.</p><p>Data collection is set up as follows:</p><ul><li>Stories posted to&nbsp;<a href="https://news.ycombinator.com/newest" target="_blank" class="wants-props-update" tabindex="0" style="color: rgb(77, 143, 168);">new</a>&nbsp;are monitored with their features being updated until max. 1h after the story being posted</li><li>Stories are monitored for up until 6h to see if they end up on the&nbsp;<a href="https://news.ycombinator.com/news" target="_blank" class="wants-props-update" tabindex="0" style="color: rgb(77, 143, 168);">frontpage</a>. Whether a story ends up on the frontpage is based on an internal algorithm but mainly driven by user engagement. I.e. do enough people upvote a story in a short enough amount of time</li><li>If stories would end up on the frontpage after the 6h mark, we will still consider them as a negative instance here</li></ul><p>Next, when you connect to the streaming data source:</p><ul><li>Stories younger than&nbsp;<em>F</em>h and older than 6h old or stories younger than&nbsp;<em>F</em>h which ended up on the frontpage already are gathered</li><li>These stories are served one by one, oldest-to-youngest</li><li><em>F</em>h starts at 12h to make sure you directly get some stories when connecting but then moves upwards so the delivery of stories becomes in sync with the gathering process described above</li><li>I.e. after going through this historical backlog stories will be delivered from the moment they appear on the frontpage (positive) or they are older than 6h (negative) (at which point the rate of instance delivery will also decline)</li></ul><p>The goal of this assignment is threefold:</p><ul><li>1 - make sure you collect a historical set of data</li><li><em>Important: get started with this as soon as possible. We will discuss Spark and text mining in more detail later on, but you can already start gathering your data</em></li><li>2 - construct a predictive model that can predict whether an article will end up one the frontpage based on what we can observe during 1h of monitoring</li><li><br></li><li>3 - show that your model can make predictions in a "deployed" setting on new stories</li></ul><p><strong style="color: rgb(48, 48, 48);">Setting up Spark</strong></p><p>Since the data set we'll work with is still relatively small, you will (luckily) not need a cluster of machines, but can run Spark locally on your machine (and save the data locally as well).</p><ul><li>First, download the ZIP file from&nbsp;<a href="https://seppe.net/aa/assignment3/spark.zip" target="_blank" class="wants-props-update" tabindex="0" style="color: rgb(77, 143, 168);">this link</a>&nbsp;and extract it somewhere, e.g. on your Desktop. This ZIP file contains a portable Spark installation using the latest stable version available at this time (3.5.1).</li><li>Second, make sure you have a Python 3 Anaconda distribution installed on your system.</li><li>Use an&nbsp;conda&nbsp;environment with Python 3 and Jupyter Notebook installed (conda install jupyter). If you are on Mac, also install the Java JDK using&nbsp;conda install openjdk&nbsp;in this environment.</li><li>Windows users: to start Spark, double-click&nbsp;letsgo-win.bat. If you get a "Microsoft Defender SmartScreen" message, press "More info" and then "Run anyway". If all goes well, a Jupyter session should open up, ready to go. If the launcher fails to find Python or Jupyter, it will provide you with an error message. Upon starting, Java might require access to your Windows firewall, which you can safely accept.</li><li>Mac users: if you're on Mac, open up a Terminal window and navigate to where you've unzipped the file and run&nbsp;letsgo-mac.sh, e.g.:</li><li>cd /Users/seppe/Desktop/spark</li><li>./letsgo-mac.sh</li><li>Make sure you are in your correct conda environment (conda activate your-env-name) where you have Python 3, Jupyter and the OpenJDK installed.</li><li>If Mac complains about the file not being executable, you might first have to enter the following command to make it executable:</li><li>chmod +x ./letsgo-mac.sh</li><li>You might also get a window popup asking if you want to install XCode. You can ignore this, as you don't need it. If all goes well, a Jupyter session should open up, ready to go.</li></ul><p>If you encounter issues, check the FAQ below first -- otherwise, feel free to e-mail me.</p><p><strong style="color: rgb(48, 48, 48);">Example notebooks</strong></p><p>Once you have Jupyter open, feel free to explore the example notebooks under "notebooks".</p><ul><li>spark_example.ipynb: A simple Spark example (calculating pi). This is a good check to see whether Spark is working correctly</li><li>spark_streaming_example.ipynb: A simple Spark Streaming example that prints out the data you'll work with. This is a good test to see whether you can receive the data</li><li>spark_streaming_example_saving.ipynb: A simple Spark Streaming example that saves the data. Use this to get started saving your historical set</li><li>spark_streaming_example_predicting.ipynb: A very na√Øve prediction approach</li><li>spark_structured_streaming_example.ipynb: An example using Spark Structured Streaming</li></ul><p><strong style="color: rgb(48, 48, 48);">Objective</strong></p><p>Using Spark, your task for this assignment is as follows:</p><ol><li>Construct a data set using the provided stream</li></ol><ul><li class="ql-indent-1">Get started with this as soon as possible</li><li class="ql-indent-1">Make sure to set up Spark using the instructions posted above</li></ul><ol><li>Construct a predictive model to predict the target (frontpage&nbsp;based on the available features)</li></ol><ul><li class="ql-indent-1">The stream is text-based with each line containing one message (one instance) formatted as a JSON dictionary</li><li class="ql-indent-1">You can use extra data and libraries if you want, but this is not required</li><li class="ql-indent-1">You are strongly encouraged to build your model using&nbsp;spark.ml&nbsp;(MLlib), but you can use&nbsp;scikit-learn&nbsp;as a fallback</li><li class="ql-indent-1">The JSON dictionary contains a&nbsp;source_text&nbsp;feature which is a best-effort plain text representation of the main content of the link that was posted (from the posted website itself).&nbsp;title,&nbsp;url,&nbsp;domain&nbsp;and&nbsp;posted_at&nbsp;are self-explanatory.&nbsp;user&nbsp;is the user who posted the story,&nbsp;comments&nbsp;is the number of comments and&nbsp;votes&nbsp;is the number of votes the story has received during the monitoring window.&nbsp;source_title&nbsp;is a best effort extraction of the title of the link that was posted (from the posted website itself)</li></ul><ol><li>Use your trained model to show you can make predictions as the stream comes in</li></ol><ul><li class="ql-indent-1">I.e. show that you can connect to the data source, preprocess/featurize incoming messages, have your model predict the label, and show it, similar to&nbsp;spark_streaming_example_predicting.ipynb&nbsp;(but hopefully using a smarter, real predictive model)</li><li class="ql-indent-1">This means that you'll need to look for a way to save and load your trained model</li><li class="ql-indent-1">The goal is not to obtain a perfect predictive accuracy, but to make sure you can set up Spark and work in a streaming environment</li></ul><p>The third part of your lab report should contain:</p><ul><li>Overview of the steps above, the source code of your programs, as well as the output after running them</li><li>Feel free to include screen shots or info on encountered challenges and how you dealt with them</li><li>Even if your solution is not fully working or not working correctly, you can still receive marks for this assignment based on what you tried and how you'd need to improve your end result</li></ul><p><strong style="color: rgb(48, 48, 48);">Further remarks</strong></p><ul><li>Get started with setting up Spark and fetching data as quickly as possible</li><li>Make sure to have enough data to train your model. New stories arrive relatively slow (the rate depends on the time of the day), but you should expect about one new story being posted every five to ten minutes or so</li><li>The data stream is line delimited with every line containing one instance in JSON format, but can be easily converted to a DataFrame (and RDD). The example notebooks give some ideas on how to do so</li><li>You can use both Spark Streaming or Spark Structured Streaming. The former is probably easier to work with</li><li>Preferably, your predictive model needs to be build using MLlib (so read documentation and tutorials). As stated above, other libraries can be used as well, as long as you can show that your model can provide predictions in real-time</li><li>Let me know in case the streaming server would crash</li></ul><p><em>You do not hand in each assignment separately, but hand in your completed lab report containing all four assignments on Sunday May 26th. For an overview of the groups, see Toledo. Note for externals (i.e. anyone who will NOT partake in the exams -- this doesn't apply to normal students): you are free to partake in (any of) the assignments individually, but not required to.</em></p><p><strong style="color: rgb(48, 48, 48);">FAQ</strong></p><ul><li>letsgo-win.bat&nbsp;doesn't start the notebook -- it says I have spaces in my path</li><li class="ql-indent-1">Try again by moving the installation to a directory without spaces.</li><li>letsgo-win.bat&nbsp;doesn't start the notebook -- it says it can't find Python</li><li class="ql-indent-1">Make sure to start the script from an Anaconda terminal / environment. I.e. the name of your Anaconda environment should show up in the prompt between parenthesis (like&nbsp;(base)). You can start an Anaconda terminal using the "Anaconda Navigator": press the small arrow next to your environment in the "Environments" tab, and pick "Open Terminal".</li><li>letsgo-win.bat&nbsp;doesn't start the notebook -- the command line window just disappears</li><li class="ql-indent-1">Most likely, the script has trouble to find your Anaconda installation. Try to start the script from an Anaconda terminal (see above) and check the output.</li><li>sc&nbsp;is not defined when I run the example notebook.</li><li class="ql-indent-1">You probably opened up Jupyter yourself rather than through the&nbsp;letsgo&nbsp;file. In this case, variables such as&nbsp;sc&nbsp;will not be initialized.</li><li>Spark starts, but the example notebooks fail and I see the following error in the terminal: "Python was not found; run without arguments to install from the Microsoft Store, or disable this shortcut from Settings &gt; Manage App Execution Aliases."</li><li class="ql-indent-1">This is Windows being silly. Go to Settings, search for "Manage App Execution Aliases", and turn off "python" and "python3".</li><li>Spark starts, but the example notebooks fail and I see the following error in the terminal: "spawn error" on Mac</li><li class="ql-indent-1">Make sure you have installed&nbsp;openjdk&nbsp;in your conda environment.</li><li>letsgo-mac.sh&nbsp;seems to hang and never start on Mac</li><li class="ql-indent-1">Make sure you have installed&nbsp;jupyter&nbsp;in your conda environment.</li><li>Spark starts, but the example notebooks fail as the Python kernel does not seem to start on Mac</li><li class="ql-indent-1">This seems to some weird Chrome related behavior. Try opening the Jupyter notebook URL using Safari.</li><li>Spark starts, but the example notebooks fail and I see the following error in the terminal: "java.io.IOException: Cannot run program "python": CreateProcess error=2, The system cannot find the file specified"</li><li class="ql-indent-1">Try editing the script and change the line containing:&nbsp;set PYSPARK_PYTHON=...&nbsp;for Windows or&nbsp;export PYSPARK_PYTHON=...&nbsp;for Mac so it says&nbsp;python3.</li><li class="ql-indent-1">If that doesn't work (especially if you're on Mac), try installing&nbsp;findspark&nbsp;and&nbsp;pyspark&nbsp;in your conda environment, e.g. see&nbsp;<a href="https://medium.com/@divya.chandana/easy-install-pyspark-in-anaconda-e2d427b3492f" target="_blank" class="wants-props-update" tabindex="0" style="color: rgb(77, 143, 168);">https://medium.com/@divya.chandana/easy-install-pyspark-in-anaconda-e2d427b3492f</a>.</li><li>Everything seems to work but I get a lot of warnings on the console or in the notebook</li><li class="ql-indent-1">Spark is very verbose. On Mac, it seems that warnings are shown in the notebook which makes it more annoying to use. If you don't like that, check&nbsp;<a href="https://stackoverflow.com/posts/70613254/revisions" target="_blank" class="wants-props-update" tabindex="0" style="color: rgb(77, 143, 168);">https://stackoverflow.com/posts/70613254/revisions</a>&nbsp;to stop Jupyter from capturing stderr</li><li>I can't save the stream... everything else seems fine</li><li class="ql-indent-1">Make sure you're calling the "saveAsTextfiles" function with "file:///" prepended to the path:&nbsp;lines.saveAsTextFiles("file:///C:/..."). Also make sure that the folder where you want to save the files exist. Note that the "saveAsTextfiles" method expects a directory name as the argument. It will automatically create a folder for each mini-batch of data.</li><li>Can I prevent the&nbsp;saveAsTextFiles&nbsp;function from creating so many directories and files?</li><li class="ql-indent-1">You can first repartition the RDD to one partition before saving it:&nbsp;lines.repartition(1).saveAsTextFiles("file:///C:/..."). To prevent multiple directories, change the trigger time to e.g.&nbsp;ssc = StreamingContext(sc, 60), though this will still create multiple directories. Setting the trigger interval higher is not really recommended, as you wouldn't want to lose data in case something goes wrong.</li><li>So if I still end up with multiple directories, how do I read them in?</li><li class="ql-indent-1">It's pretty easy to <b>loop over subdirectories in Python.</b> Alternatively, the&nbsp;sc.textFile&nbsp;command is pretty smart and can parse through multiple files in one go.</li><li>Is it normal all my folders only contain&nbsp;_SUCCESS&nbsp;files but no actual data files?</li><li class="ql-indent-1">That depends. A&nbsp;_SUCCESS&nbsp;file indicates that the mini-batch was saved correctly.&nbsp;part-*&nbsp;files contain the actual data. And files ending with&nbsp;.crc&nbsp;contain a checksum. It's normal if not all of your folders contain&nbsp;part-*&nbsp;data, as it might be that no data was received in that time frame. However, if none of your folders are having data and you've been trying to run the code for some time, something else has gone wrong. Try the&nbsp;spark_streaming_example.ipynb&nbsp;notebook to verify whether you're at least receiving data at all.</li><li>Is there a way how I can monitor Spark?</li><li class="ql-indent-1">Yes, go to&nbsp;<a href="http://127.0.0.1:4040/" target="_blank" class="wants-props-update" tabindex="0" style="color: rgb(77, 143, 168);">http://127.0.0.1:4040/</a>&nbsp;in your browser while Spark is running and you'll get access to a monitoring dashboard. Under the "Environment" tab, you should be able to find a "spark.speculation" entry for instance w.r.t. the question above. Under "Jobs", "Stage", and "Streaming", you can get more info on how things are going.</li><li>I'm trying to convert my saved files to a DataFrame, but Spark complains for some files?</li><li class="ql-indent-1">Data is always messy, especially the ones provided by this instructor. Make sure you can handle badly formatted lines and discard them.</li><li>My stream crashes after a while with an "RDD is empty" error...</li><li class="ql-indent-1">Make sure you're checking for empty RDDs, e.g.&nbsp;if rdd.isEmpty(): return.</li><li>I've managed to create a model. When I try to apply it on the stream, Spark crashes with a Hive / Derby error, e.g. when I try to .load() my model(s) or once the first RDD arrives</li><li class="ql-indent-1">Check the example notebooks for ideas on how to load in your model in "globals()" once.</li><li>When I call&nbsp;ssc_t.stop(), Spark never seems to stop the stream</li><li class="ql-indent-1">You can try changing&nbsp;stopGraceFully=True&nbsp;to&nbsp;False. Even then, Spark might not want to stop its stream processing pipeline in case you're doing a lot with the incoming data, preventing Spark from cleaning up. Try decreasing the trigger time, or simply restart the Jupyter kernel to start over.</li><li>Spark complains that only one StreamingContext can be active at a time (and other general "it doesn't work anymore" questions)</li><li class="ql-indent-1">In case of trouble, a good idea is always to (save and) close all running notebooks and start again fresh. Spark doesn't like having multiple StreamingContext notebooks running, so it is best to only have one notebook running at a time. (Closing a tab with a notebook does not mean that the kernel is stopped, check the "Running" tab on the Jupyter main page.)</li><li>Why do I receive the same instances (or: why do I have instances twice) when reconnecting?</li><li class="ql-indent-1">To make sure you are served some articles right away, the stream server starts from articles that were posted 12h back and works its way back to the 6h before mark. You can remove instances based on the&nbsp;aid.</li><li>Can I use R?</li><li class="ql-indent-1">There are two main Spark R packages being maintained right now:&nbsp;SparkR&nbsp;(the official one) and&nbsp;sparklyr&nbsp;(from the folks at RStudio and fits better with the tidyverse). You can try using these, but you'll have to do some setting up in order so R can find your Spark installation. I'd strongly recommend using Python.</li></ul></div>
